{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOtqX8LNrVnq75qmWQcAYk2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a70b9c7db58b4c1197e0693ead1c0868":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4325d7ff3924cdbb84703365e3a7e10","IPY_MODEL_516a9dc0929645d284befd434d2797b4","IPY_MODEL_f8fd5b4f02f94cb6af5e9e18c2608054"],"layout":"IPY_MODEL_2b95462e0c08446f880edbe4c357e4b9"}},"a4325d7ff3924cdbb84703365e3a7e10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a97af82aef9347b285e238d2f963f9b0","placeholder":"​","style":"IPY_MODEL_54776e3f18f446a892b2d88028473b2a","value":"Downloading builder script: "}},"516a9dc0929645d284befd434d2797b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a700d3999f54c26adfbd79d8aa673a9","max":2472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a2554ee07674cc3803495c29beb24e4","value":2472}},"f8fd5b4f02f94cb6af5e9e18c2608054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ec12b84c2645e7b27a17d960c02edb","placeholder":"​","style":"IPY_MODEL_077bc3e3b47941e096b1130fe5a6b4ff","value":" 6.33k/? [00:00&lt;00:00, 169kB/s]"}},"2b95462e0c08446f880edbe4c357e4b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97af82aef9347b285e238d2f963f9b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54776e3f18f446a892b2d88028473b2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a700d3999f54c26adfbd79d8aa673a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a2554ee07674cc3803495c29beb24e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4ec12b84c2645e7b27a17d960c02edb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"077bc3e3b47941e096b1130fe5a6b4ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n","!pip install tokenizer\n","!pip install transformers[torch]\n","!pip install seqeval"],"metadata":{"id":"Zx-H8vGUlrQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","import numpy as np\n","from transformers import BertTokenizerFast\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n"],"metadata":{"id":"AfdabH6imKCj","executionInfo":{"status":"ok","timestamp":1708230232502,"user_tz":-330,"elapsed":9201,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"conll2003\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCN3FQ5OnwGM","executionInfo":{"status":"ok","timestamp":1708230235748,"user_tz":-330,"elapsed":3265,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"8cc36445-b6d5-47fb-be84-947d81af759e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av4Bt4nYoMex","executionInfo":{"status":"ok","timestamp":1708230244736,"user_tz":-330,"elapsed":397,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"3dd3236b-b658-4ad4-889a-464301fc4035"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 14041\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3250\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3453\n","    })\n","})"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["dataset['train']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45hDQBi_olLJ","executionInfo":{"status":"ok","timestamp":1708230245323,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"d7a2e715-555b-49b4-bec7-b685b6383834"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","    num_rows: 14041\n","})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAruxDlzqK1o","executionInfo":{"status":"ok","timestamp":1708230245812,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"a1ee7f27-dda3-4d55-ef03-e6aeb95a3847"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# pos_tags: part of speech\n","# ner_tags: Named entity recognition"],"metadata":{"id":"yM9EkZgCqMxw","executionInfo":{"status":"ok","timestamp":1708230246934,"user_tz":-330,"elapsed":3,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df_train = dataset['train'].to_pandas()"],"metadata":{"id":"xjswTXVWp4-7","executionInfo":{"status":"ok","timestamp":1708230247457,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["df_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"8qeOG2jHp9IK","executionInfo":{"status":"ok","timestamp":1708230248229,"user_tz":-330,"elapsed":9,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"f0eed596-7e4d-4ac0-f558-03702ea3d9b4"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id                                             tokens  \\\n","0          0  [EU, rejects, German, call, to, boycott, Briti...   \n","1          1                                 [Peter, Blackburn]   \n","2          2                             [BRUSSELS, 1996-08-22]   \n","3          3  [The, European, Commission, said, on, Thursday...   \n","4          4  [Germany, 's, representative, to, the, Europea...   \n","...      ...                                                ...   \n","14036  14036                                    [on, Friday, :]   \n","14037  14037                                    [Division, two]   \n","14038  14038                          [Plymouth, 2, Preston, 1]   \n","14039  14039                                  [Division, three]   \n","14040  14040                           [Swansea, 1, Lincoln, 2]   \n","\n","                                                pos_tags  \\\n","0                    [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n","1                                               [22, 22]   \n","2                                               [22, 11]   \n","3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...   \n","4      [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...   \n","...                                                  ...   \n","14036                                        [15, 22, 8]   \n","14037                                           [21, 11]   \n","14038                                   [21, 11, 22, 11]   \n","14039                                           [21, 11]   \n","14040                                   [21, 11, 22, 11]   \n","\n","                                              chunk_tags  \\\n","0                    [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n","1                                               [11, 12]   \n","2                                               [11, 12]   \n","3      [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...   \n","4      [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...   \n","...                                                  ...   \n","14036                                        [13, 11, 0]   \n","14037                                           [11, 12]   \n","14038                                   [11, 12, 12, 12]   \n","14039                                           [11, 12]   \n","14040                                   [11, 12, 12, 12]   \n","\n","                                                ner_tags  \n","0                            [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n","1                                                 [1, 2]  \n","2                                                 [5, 0]  \n","3      [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...  \n","4      [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...  \n","...                                                  ...  \n","14036                                          [0, 0, 0]  \n","14037                                             [0, 0]  \n","14038                                       [3, 0, 3, 0]  \n","14039                                             [0, 0]  \n","14040                                       [3, 0, 3, 0]  \n","\n","[14041 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-079cdd9e-1454-4a64-9f37-6213b843eccc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tokens</th>\n","      <th>pos_tags</th>\n","      <th>chunk_tags</th>\n","      <th>ner_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>[EU, rejects, German, call, to, boycott, Briti...</td>\n","      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n","      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n","      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>[Peter, Blackburn]</td>\n","      <td>[22, 22]</td>\n","      <td>[11, 12]</td>\n","      <td>[1, 2]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>[BRUSSELS, 1996-08-22]</td>\n","      <td>[22, 11]</td>\n","      <td>[11, 12]</td>\n","      <td>[5, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>[The, European, Commission, said, on, Thursday...</td>\n","      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 3...</td>\n","      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 1...</td>\n","      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>[Germany, 's, representative, to, the, Europea...</td>\n","      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 2...</td>\n","      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 1...</td>\n","      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14036</th>\n","      <td>14036</td>\n","      <td>[on, Friday, :]</td>\n","      <td>[15, 22, 8]</td>\n","      <td>[13, 11, 0]</td>\n","      <td>[0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>14037</th>\n","      <td>14037</td>\n","      <td>[Division, two]</td>\n","      <td>[21, 11]</td>\n","      <td>[11, 12]</td>\n","      <td>[0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>14038</th>\n","      <td>14038</td>\n","      <td>[Plymouth, 2, Preston, 1]</td>\n","      <td>[21, 11, 22, 11]</td>\n","      <td>[11, 12, 12, 12]</td>\n","      <td>[3, 0, 3, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>14039</th>\n","      <td>14039</td>\n","      <td>[Division, three]</td>\n","      <td>[21, 11]</td>\n","      <td>[11, 12]</td>\n","      <td>[0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>14040</th>\n","      <td>14040</td>\n","      <td>[Swansea, 1, Lincoln, 2]</td>\n","      <td>[21, 11, 22, 11]</td>\n","      <td>[11, 12, 12, 12]</td>\n","      <td>[3, 0, 3, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14041 rows × 5 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-079cdd9e-1454-4a64-9f37-6213b843eccc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-079cdd9e-1454-4a64-9f37-6213b843eccc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-079cdd9e-1454-4a64-9f37-6213b843eccc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2c2bd675-7880-4314-a13e-aace9adebcc5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c2bd675-7880-4314-a13e-aace9adebcc5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2c2bd675-7880-4314-a13e-aace9adebcc5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_fef691b7-5b09-4f6b-a5bc-275c6be04684\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_fef691b7-5b09-4f6b-a5bc-275c6be04684 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_train');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_train","repr_error":"unhashable type: 'numpy.ndarray'"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["dataset['train'].features['ner_tags']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxrQ3-6dqWL4","executionInfo":{"status":"ok","timestamp":1708230248772,"user_tz":-330,"elapsed":7,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"45836353-d5e1-40df-fa96-e5fa6e5c30c9"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["dataset['train'].features['pos_tags']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtyJKOsOqcGD","executionInfo":{"status":"ok","timestamp":1708230248772,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"3cdf21d8-1b9c-433f-9421-5cb51ca8ecc9"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["dataset['train'].features['chunk_tags']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LWeCZG0rTYr","executionInfo":{"status":"ok","timestamp":1708230249730,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"1b2adb6e-8eca-46a0-bd4a-116df3fcdd84"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["dataset['train'].description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"32LS_kfvYFal","executionInfo":{"status":"ok","timestamp":1708230250262,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"5f9350b6-e110-4e72-82d0-e225770ce95f"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["example_text = dataset['train'][0]\n","example_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4hOhbMnYLhn","executionInfo":{"status":"ok","timestamp":1708230250764,"user_tz":-330,"elapsed":6,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"32eaffea-8640-4afe-d674-6a048558ba20"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["example_text[\"tokens\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nglWkpYFYLkw","executionInfo":{"status":"ok","timestamp":1708230251254,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"5c09ba0f-1f77-41b1-cbbd-a774c48efa73"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["model_name = 'bert-base-uncased'\n","\n","tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","\n","tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True) # converting tokens to ids\n","print(tokenized_input)\n","print(tokenized_input['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WTebQ5XYRNZ","executionInfo":{"status":"ok","timestamp":1708230251254,"user_tz":-330,"elapsed":3,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"c41a40f4-c72c-47d2-ea5c-749b2cb822de"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]\n"]}]},{"cell_type":"code","source":["tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])# converting ids to tokens\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbjY8LWYYRQm","executionInfo":{"status":"ok","timestamp":1708230251732,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"eec6852f-8cb4-4181-c1e7-20ed928760de"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'eu',\n"," 'rejects',\n"," 'german',\n"," 'call',\n"," 'to',\n"," 'boycott',\n"," 'british',\n"," 'lamb',\n"," '.',\n"," '[SEP]']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["word_ids = tokenized_input.word_ids() # index of each ids (tokens)\n","\n","print(word_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrkYyAe0YRT9","executionInfo":{"status":"ok","timestamp":1708230252252,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"6cbf9f04-de12-410c-e295-4a066fac364d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"]}]},{"cell_type":"code","source":["example_text[\"ner_tags\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgz6vFe8YLpB","executionInfo":{"status":"ok","timestamp":1708230252726,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"a13789c8-12dc-46b1-e266-7894c6e3ae85"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3, 0, 7, 0, 0, 0, 7, 0, 0]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["for i, label in enumerate(example_text[\"ner_tags\"]):\n","  print(i,label)"],"metadata":{"id":"wONhlgseyHeN","executionInfo":{"status":"ok","timestamp":1708230253730,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"383503d7-cc34-4942-c66e-a97e7d767ae1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["0 3\n","1 0\n","2 7\n","3 0\n","4 0\n","5 0\n","6 7\n","7 0\n","8 0\n"]}]},{"cell_type":"code","source":["def tokenize_and_align_labels(examples, label_all_tokens=True):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","    labels = []\n","    for i, label in enumerate(examples[\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        # word_ids() => Return a list mapping the tokens\n","        # to their actual word in the initial sentence.\n","        # It Returns a list indicating the word corresponding to each token.\n","        previous_word_idx = None\n","        label_ids = []\n","        # Special tokens like `` and `<\\s>` are originally mapped to None\n","        # We need to set the label to -100 so they are automatically ignored in the loss function.\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                # set –100 as the label for these special tokens\n","                label_ids.append(-100)\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            elif word_idx != previous_word_idx:\n","                # if current word_idx is != prev then its the most regular case\n","                # and add the corresponding token\n","                label_ids.append(label[word_idx])\n","            else:\n","                # to take care of sub-words which have the same word_idx\n","                # set -100 as well for them, but only if label_all_tokens == False\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","                # mask the subword representations after the first subword\n","\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"],"metadata":{"id":"uSi3diRgzRuy","executionInfo":{"status":"ok","timestamp":1708230254244,"user_tz":-330,"elapsed":3,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["dataset['train'][4:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctT3CAJu1U-n","executionInfo":{"status":"ok","timestamp":1708230255668,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"f0e0de83-4fbe-45ca-d15e-0b2c2fa94d0f"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': ['4'],\n"," 'tokens': [['Germany',\n","   \"'s\",\n","   'representative',\n","   'to',\n","   'the',\n","   'European',\n","   'Union',\n","   \"'s\",\n","   'veterinary',\n","   'committee',\n","   'Werner',\n","   'Zwingmann',\n","   'said',\n","   'on',\n","   'Wednesday',\n","   'consumers',\n","   'should',\n","   'buy',\n","   'sheepmeat',\n","   'from',\n","   'countries',\n","   'other',\n","   'than',\n","   'Britain',\n","   'until',\n","   'the',\n","   'scientific',\n","   'advice',\n","   'was',\n","   'clearer',\n","   '.']],\n"," 'pos_tags': [[22,\n","   27,\n","   21,\n","   35,\n","   12,\n","   22,\n","   22,\n","   27,\n","   16,\n","   21,\n","   22,\n","   22,\n","   38,\n","   15,\n","   22,\n","   24,\n","   20,\n","   37,\n","   21,\n","   15,\n","   24,\n","   16,\n","   15,\n","   22,\n","   15,\n","   12,\n","   16,\n","   21,\n","   38,\n","   17,\n","   7]],\n"," 'chunk_tags': [[11,\n","   11,\n","   12,\n","   13,\n","   11,\n","   12,\n","   12,\n","   11,\n","   12,\n","   12,\n","   12,\n","   12,\n","   21,\n","   13,\n","   11,\n","   12,\n","   21,\n","   22,\n","   11,\n","   13,\n","   11,\n","   1,\n","   13,\n","   11,\n","   17,\n","   11,\n","   12,\n","   12,\n","   21,\n","   1,\n","   0]],\n"," 'ner_tags': [[5,\n","   0,\n","   0,\n","   0,\n","   0,\n","   3,\n","   4,\n","   0,\n","   0,\n","   0,\n","   1,\n","   2,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   5,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0]]}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["test = [['Germany',\n","   \"'s\",\n","   'representative',\n","   'to',\n","   'the',\n","   'European',\n","   'Union',\n","   \"'s\",\n","   'veterinary',\n","   'committee',\n","   'Werner',\n","   'Zwingmann',\n","   'said',\n","   'on',\n","   'Wednesday',\n","   'consumers',\n","   'should',\n","   'buy',\n","   'sheepmeat',\n","   'from',\n","   'countries',\n","   'other',\n","   'than',\n","   'Britain',\n","   'until',\n","   'the',\n","   'scientific',\n","   'advice',\n","   'was',\n","   'clearer',\n","   '.']]\n","len(test[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HN60IE9LZXVW","executionInfo":{"status":"ok","timestamp":1708230256106,"user_tz":-330,"elapsed":2,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"0329171e-2c09-4200-9e94-4f4bbb94207b"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["31"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["df_train[4:5]['tokens'].values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5hBSJhXZqdX","executionInfo":{"status":"ok","timestamp":1708230256725,"user_tz":-330,"elapsed":3,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"41c4950e-8fb9-497d-efba-ac4c938f4b9f"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([array(['Germany', \"'s\", 'representative', 'to', 'the', 'European',\n","              'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann',\n","              'said', 'on', 'Wednesday', 'consumers', 'should', 'buy',\n","              'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain',\n","              'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'],\n","             dtype=object)                                                    ],\n","      dtype=object)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["q=tokenize_and_align_labels(dataset[\"train\"][4:5])\n","q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h06oww62zzI3","executionInfo":{"status":"ok","timestamp":1708230258774,"user_tz":-330,"elapsed":360,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"fee617b9-e3a9-4769-d2b5-37daa310cf6b"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["'''input_ids: These are the tokenized input sequences converted into numerical indices. Each token in the input text is mapped to its corresponding token ID in the model's vocabulary. These token IDs are what the model actually\n","processes during training or inference.\n","\n","token_type_ids: In models like BERT that accept two separate sequences as input (e.g., for question answering tasks where you have both the question and the context), this tensor distinguishes between the two sequences.\n","For single-sequence tasks, like text classification or named entity recognition (NER), all token type IDs are typically set to the same value, usually 0.\n","\n","attention_mask: This mask indicates to the model which tokens should be attended to and which ones should be ignored. It has the same shape as input_ids and consists of 1s and 0s. A value of 1 indicates that the corresponding\n","token should be attended to, while a value of 0 indicates that the corresponding token should be ignored (usually padding tokens). This mask helps the model focus only on the relevant parts of the input sequence.\n","\n","labels: In tasks like sequence labeling (e.g., NER), this contains the target labels corresponding to each token in the input sequence. Each label represents the classification or prediction the model should make for the\n","corresponding token. For tokens that are part of the input text, the label IDs are set accordingly. For special tokens or tokens introduced during tokenization (like padding tokens), the label IDs are typically set to a special\n"," value, often -100, to indicate that they should be ignored during training.\n","\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"fiMwOoBzaFpb","executionInfo":{"status":"ok","timestamp":1708230259359,"user_tz":-330,"elapsed":8,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"6338589f-22ef-475a-d231-fc4d147959ca"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"input_ids: These are the tokenized input sequences converted into numerical indices. Each token in the input text is mapped to its corresponding token ID in the model's vocabulary. These token IDs are what the model actually \\nprocesses during training or inference.\\n\\ntoken_type_ids: In models like BERT that accept two separate sequences as input (e.g., for question answering tasks where you have both the question and the context), this tensor distinguishes between the two sequences. \\nFor single-sequence tasks, like text classification or named entity recognition (NER), all token type IDs are typically set to the same value, usually 0.\\n\\nattention_mask: This mask indicates to the model which tokens should be attended to and which ones should be ignored. It has the same shape as input_ids and consists of 1s and 0s. A value of 1 indicates that the corresponding \\ntoken should be attended to, while a value of 0 indicates that the corresponding token should be ignored (usually padding tokens). This mask helps the model focus only on the relevant parts of the input sequence.\\n\\nlabels: In tasks like sequence labeling (e.g., NER), this contains the target labels corresponding to each token in the input sequence. Each label represents the classification or prediction the model should make for the \\ncorresponding token. For tokens that are part of the input text, the label IDs are set accordingly. For special tokens or tokens introduced during tokenization (like padding tokens), the label IDs are typically set to a special\\n value, often -100, to indicate that they should be ignored during training.\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["len(q['input_ids'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmrTm75DZd0t","executionInfo":{"status":"ok","timestamp":1708230259710,"user_tz":-330,"elapsed":6,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"09108772-279a-48b0-a784-fb92e925f11e"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["39"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["## why the lengths are different\n","\n","'''\n","The difference in lengths between the input_ids and labels arrays is likely due to tokenization.\n","\n","In NLP tasks, tokenization involves breaking down the input text into smaller units called tokens. These tokens may not directly correspond to the words in the original text, especially when dealing with languages like English where words can have prefixes, suffixes, or punctuation attached.\n","\n","In your tokenize_and_align_labels function, you're tokenizing the input text and aligning the labels accordingly. This alignment ensures that each token in the input text has a corresponding label, even if the tokenization splits a word into multiple tokens.\n","\n","The discrepancy in lengths between input_ids and labels could occur because tokenization may split some words into subwords, resulting in more tokens than there are labels. To ensure alignment, special tokens are introduced, and labels for these tokens are set to -100, indicating that they should be ignored during training.\n","\n","So, in your example, the difference in lengths is due to tokenization splitting some words into multiple tokens, resulting in more tokens than there are labels, and the additional tokens have label IDs of -100.\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"8w8flUFwaw84","executionInfo":{"status":"ok","timestamp":1708230259710,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"fbfe3913-792d-4a32-ee1d-48ff9e89c9b3"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nThe difference in lengths between the input_ids and labels arrays is likely due to tokenization.\\n\\nIn NLP tasks, tokenization involves breaking down the input text into smaller units called tokens. These tokens may not directly correspond to the words in the original text, especially when dealing with languages like English where words can have prefixes, suffixes, or punctuation attached.\\n\\nIn your tokenize_and_align_labels function, you're tokenizing the input text and aligning the labels accordingly. This alignment ensures that each token in the input text has a corresponding label, even if the tokenization splits a word into multiple tokens.\\n\\nThe discrepancy in lengths between input_ids and labels could occur because tokenization may split some words into subwords, resulting in more tokens than there are labels. To ensure alignment, special tokens are introduced, and labels for these tokens are set to -100, indicating that they should be ignored during training.\\n\\nSo, in your example, the difference in lengths is due to tokenization splitting some words into multiple tokens, resulting in more tokens than there are labels, and the additional tokens have label IDs of -100.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n","    print(f\"{token:_<40} {label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uu9nszZg1M8N","executionInfo":{"status":"ok","timestamp":1708230261206,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"799d5358-0238-4251-f1c7-7fcf1bcc5bf3"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]___________________________________ -100\n","germany_________________________________ 5\n","'_______________________________________ 0\n","s_______________________________________ 0\n","representative__________________________ 0\n","to______________________________________ 0\n","the_____________________________________ 0\n","european________________________________ 3\n","union___________________________________ 4\n","'_______________________________________ 0\n","s_______________________________________ 0\n","veterinary______________________________ 0\n","committee_______________________________ 0\n","werner__________________________________ 1\n","z_______________________________________ 2\n","##wing__________________________________ 2\n","##mann__________________________________ 2\n","said____________________________________ 0\n","on______________________________________ 0\n","wednesday_______________________________ 0\n","consumers_______________________________ 0\n","should__________________________________ 0\n","buy_____________________________________ 0\n","sheep___________________________________ 0\n","##me____________________________________ 0\n","##at____________________________________ 0\n","from____________________________________ 0\n","countries_______________________________ 0\n","other___________________________________ 0\n","than____________________________________ 0\n","britain_________________________________ 5\n","until___________________________________ 0\n","the_____________________________________ 0\n","scientific______________________________ 0\n","advice__________________________________ 0\n","was_____________________________________ 0\n","clearer_________________________________ 0\n","._______________________________________ 0\n","[SEP]___________________________________ -100\n"]}]},{"cell_type":"code","source":["## Applying on entire data\n","tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"],"metadata":{"id":"Af9Zi8j52GZO","executionInfo":{"status":"ok","timestamp":1708230263240,"user_tz":-330,"elapsed":338,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets[\"train\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbW2Pu7_cWA8","executionInfo":{"status":"ok","timestamp":1708230263739,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"b199a122-2f0f-4111-b8b0-0ee7cd91f18a"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n"," 'input_ids': [101,\n","  7327,\n","  19164,\n","  2446,\n","  2655,\n","  2000,\n","  17757,\n","  2329,\n","  12559,\n","  1012,\n","  102],\n"," 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\",num_labels=9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqtRI5u2cZGP","executionInfo":{"status":"ok","timestamp":1708230267205,"user_tz":-330,"elapsed":2173,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"c60536b0-cee5-4fa4-a8b0-47c018ac51f3"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","args = TrainingArguments(\n","\"test-ner\",\n","evaluation_strategy = \"epoch\",\n","learning_rate=2e-5,\n","per_device_train_batch_size=16,\n","per_device_eval_batch_size=16,\n","num_train_epochs=1,\n","weight_decay=0.01\n",")"],"metadata":{"id":"0R_piYvacczk","executionInfo":{"status":"ok","timestamp":1708230272007,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import datasets"],"metadata":{"id":"tODoiieLduj2","executionInfo":{"status":"ok","timestamp":1708230376316,"user_tz":-330,"elapsed":5,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["data_collator=DataCollatorForTokenClassification(tokenizer)\n","\n","metric=datasets.load_metric(\"seqeval\")\n","\n","def compute_metrics(eval_preds):\n","    pred_logits, labels = eval_preds\n","\n","    pred_logits = np.argmax(pred_logits, axis=2)\n","    # the logits and the probabilities are in the same order,\n","    # so we don’t need to apply the softmax\n","\n","    # We remove all the values where the label is -100\n","    predictions = [\n","        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(pred_logits, labels)\n","    ]\n","\n","    true_labels = [\n","      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","       for prediction, label in zip(pred_logits, labels)\n","   ]\n","    results = metric.compute(predictions=predictions, references=true_labels)\n","\n","    return {\n","          \"precision\": results[\"overall_precision\"],\n","          \"recall\": results[\"overall_recall\"],\n","          \"f1\": results[\"overall_f1\"],\n","          \"accuracy\": results[\"overall_accuracy\"],\n","  }"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["a70b9c7db58b4c1197e0693ead1c0868","a4325d7ff3924cdbb84703365e3a7e10","516a9dc0929645d284befd434d2797b4","f8fd5b4f02f94cb6af5e9e18c2608054","2b95462e0c08446f880edbe4c357e4b9","a97af82aef9347b285e238d2f963f9b0","54776e3f18f446a892b2d88028473b2a","8a700d3999f54c26adfbd79d8aa673a9","6a2554ee07674cc3803495c29beb24e4","b4ec12b84c2645e7b27a17d960c02edb","077bc3e3b47941e096b1130fe5a6b4ff"]},"id":"XW2bfURIdfWb","executionInfo":{"status":"ok","timestamp":1708230380829,"user_tz":-330,"elapsed":963,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"b456d9f9-4664-4a8f-9bb0-cabd733c6e8e"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-207f397bb7cd>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  metric=datasets.load_metric(\"seqeval\")\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/seqeval/seqeval.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70b9c7db58b4c1197e0693ead1c0868"}},"metadata":{}}]},{"cell_type":"code","source":["Trainer(\n","   model,\n","   args,\n","   train_dataset=tokenized_datasets[\"train\"],\n","   eval_dataset=tokenized_datasets[\"validation\"],\n","   data_collator=data_collator,  # The function to use to form a batch from a list of elements of train_dataset or eval_dataset.\n","   tokenizer=tokenizer,\n","   compute_metrics=compute_metrics\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CuHitqtcfx-","executionInfo":{"status":"ok","timestamp":1708230383762,"user_tz":-330,"elapsed":7,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"3b69f94d-2f30-45ca-eed0-dec6433c9c82"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<transformers.trainer.Trainer at 0x79c9635ec670>"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["example=dataset['train'][0]\n","example"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j81jEfVidZQn","executionInfo":{"status":"ok","timestamp":1708230434391,"user_tz":-330,"elapsed":9,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"1f587896-97a3-4cc1-c702-7800b64dadd1"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n","\n","label_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hG0OeNdCd0oT","executionInfo":{"status":"ok","timestamp":1708230434392,"user_tz":-330,"elapsed":7,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"581cba46-fe76-4a84-bf32-b33ea8ead563"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["for i in example[\"ner_tags\"]:\n","  print(i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GXrJFuNOd41H","executionInfo":{"status":"ok","timestamp":1708230436489,"user_tz":-330,"elapsed":348,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"477add8f-042d-453f-b657-e79daab8cefb"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","0\n","7\n","0\n","0\n","0\n","7\n","0\n","0\n"]}]},{"cell_type":"code","source":["labels = [label_list[i] for i in example[\"ner_tags\"]]\n","labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urGL0SSGd-EP","executionInfo":{"status":"ok","timestamp":1708230454768,"user_tz":-330,"elapsed":364,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"9069f5c7-81fb-4f17-c9a0-2fd6a9d9a410"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["metric.compute(predictions=[labels],references=[labels])  # just testing the compute metric"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05DC3EZUeCjz","executionInfo":{"status":"ok","timestamp":1708230466641,"user_tz":-330,"elapsed":345,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"9adbfd06-f189-4a3a-834d-c40cbe19bf5a"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n"," 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["trainer=Trainer(\n","   model,\n","   args,\n","   train_dataset=tokenized_datasets[\"train\"],\n","   eval_dataset=tokenized_datasets[\"validation\"],\n","   data_collator=data_collator,\n","   tokenizer=tokenizer,\n","   compute_metrics=compute_metrics\n",")"],"metadata":{"id":"yyp9TMEPeFbF","executionInfo":{"status":"ok","timestamp":1708230499784,"user_tz":-330,"elapsed":377,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"id":"fAJmjs2VeNlx","executionInfo":{"status":"ok","timestamp":1708230667123,"user_tz":-330,"elapsed":162282,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"223f6f91-5f5d-45fd-f9b7-26b66c9d74e9"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='878' max='878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [878/878 02:39, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.228100</td>\n","      <td>0.071406</td>\n","      <td>0.904215</td>\n","      <td>0.924041</td>\n","      <td>0.914020</td>\n","      <td>0.980444</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=878, training_loss=0.1671213832151374, metrics={'train_runtime': 161.862, 'train_samples_per_second': 86.747, 'train_steps_per_second': 5.424, 'total_flos': 342221911376202.0, 'train_loss': 0.1671213832151374, 'epoch': 1.0})"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# Saving the model and tokenizer\n","\n","model.save_pretrained(\"ner_model\")\n","\n","tokenizer.save_pretrained(\"tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N93Wqmu5eO0u","executionInfo":{"status":"ok","timestamp":1708231003406,"user_tz":-330,"elapsed":2536,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"557fcf2c-dde2-4231-f372-2d5314c9576a"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/vocab.txt',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["label_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1yAw6OEfvZZ","executionInfo":{"status":"ok","timestamp":1708231023075,"user_tz":-330,"elapsed":377,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"1217a3da-1a3b-4040-ea3c-3e375ca8cf05"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["**inference part starts**"],"metadata":{"id":"3Dqet6QMgSAn"}},{"cell_type":"code","source":["id2label = {\n","    str(i): label for i,label in enumerate(label_list)\n","}\n","\n","id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExqHmZUugNUu","executionInfo":{"status":"ok","timestamp":1708231056981,"user_tz":-330,"elapsed":10,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"f61ab587-50e0-4106-aa96-06a587956b96"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'0': 'O',\n"," '1': 'B-PER',\n"," '2': 'I-PER',\n"," '3': 'B-ORG',\n"," '4': 'I-ORG',\n"," '5': 'B-LOC',\n"," '6': 'I-LOC',\n"," '7': 'B-MISC',\n"," '8': 'I-MISC'}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["label2id = {\n","    label: str(i) for i,label in enumerate(label_list)\n","}\n","\n","label2id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWm_erCKgUCP","executionInfo":{"status":"ok","timestamp":1708231066052,"user_tz":-330,"elapsed":35,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"bb43c6f0-e240-46a7-b01e-2c54943a97f6"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'O': '0',\n"," 'B-PER': '1',\n"," 'I-PER': '2',\n"," 'B-ORG': '3',\n"," 'I-ORG': '4',\n"," 'B-LOC': '5',\n"," 'I-LOC': '6',\n"," 'B-MISC': '7',\n"," 'I-MISC': '8'}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["import json\n","config=json.load(open(\"/content/ner_model/config.json\"))\n","config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuW6l9_sgXz9","executionInfo":{"status":"ok","timestamp":1708231084237,"user_tz":-330,"elapsed":958,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"1c76900e-d0cd-4e46-bc46-02f1948ad6b5"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'_name_or_path': 'bert-base-uncased',\n"," 'architectures': ['BertForTokenClassification'],\n"," 'attention_probs_dropout_prob': 0.1,\n"," 'classifier_dropout': None,\n"," 'gradient_checkpointing': False,\n"," 'hidden_act': 'gelu',\n"," 'hidden_dropout_prob': 0.1,\n"," 'hidden_size': 768,\n"," 'id2label': {'0': 'LABEL_0',\n","  '1': 'LABEL_1',\n","  '2': 'LABEL_2',\n","  '3': 'LABEL_3',\n","  '4': 'LABEL_4',\n","  '5': 'LABEL_5',\n","  '6': 'LABEL_6',\n","  '7': 'LABEL_7',\n","  '8': 'LABEL_8'},\n"," 'initializer_range': 0.02,\n"," 'intermediate_size': 3072,\n"," 'label2id': {'LABEL_0': 0,\n","  'LABEL_1': 1,\n","  'LABEL_2': 2,\n","  'LABEL_3': 3,\n","  'LABEL_4': 4,\n","  'LABEL_5': 5,\n","  'LABEL_6': 6,\n","  'LABEL_7': 7,\n","  'LABEL_8': 8},\n"," 'layer_norm_eps': 1e-12,\n"," 'max_position_embeddings': 512,\n"," 'model_type': 'bert',\n"," 'num_attention_heads': 12,\n"," 'num_hidden_layers': 12,\n"," 'pad_token_id': 0,\n"," 'position_embedding_type': 'absolute',\n"," 'torch_dtype': 'float32',\n"," 'transformers_version': '4.35.2',\n"," 'type_vocab_size': 2,\n"," 'use_cache': True,\n"," 'vocab_size': 30522}"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["**Now we will change the id2labels and labels2ids in config.json**"],"metadata":{"id":"U5m1NdyZgeSz"}},{"cell_type":"code","source":["config[\"id2label\"] = id2label\n","config[\"label2id\"] = label2id\n","\n","json.dump(config,open(\"/content/ner_model/config.json\",\"w\"))"],"metadata":{"id":"kxUyvuGIgcBt","executionInfo":{"status":"ok","timestamp":1708231143504,"user_tz":-330,"elapsed":338,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["**Loading the Model**"],"metadata":{"id":"axT9hCelgsmD"}},{"cell_type":"code","source":["model_fine_tuned=AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n","model_fine_tuned"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1b78bQxygqtL","executionInfo":{"status":"ok","timestamp":1708231176885,"user_tz":-330,"elapsed":2270,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"43594d62-f687-4b8c-b69e-7dee2091d351"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["**transformer pipeline**"],"metadata":{"id":"_XIUuwFvg05P"}},{"cell_type":"code","source":["from transformers import pipeline\n","nlp_pipeline=pipeline(\"ner\",model=model_fine_tuned,tokenizer=tokenizer) # model and tokenizer given here\n","nlp_pipeline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05lDHzl1g0aU","executionInfo":{"status":"ok","timestamp":1708231227624,"user_tz":-330,"elapsed":4,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"ee96076e-349d-4cf1-972a-ce2bf8accd16"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x79c96096a7a0>"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["example=\"sudhanshu kumar is a foundar of iNeuron\""],"metadata":{"id":"KOrrvE1ygyas","executionInfo":{"status":"ok","timestamp":1708231228219,"user_tz":-330,"elapsed":13,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["nlp_pipeline(example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vK1Rumng8Np","executionInfo":{"status":"ok","timestamp":1708231232182,"user_tz":-330,"elapsed":592,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"b2d13c13-7d40-40dc-c985-404cc6a26514"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-PER',\n","  'score': 0.98871547,\n","  'index': 1,\n","  'word': 'sud',\n","  'start': 0,\n","  'end': 3},\n"," {'entity': 'B-PER',\n","  'score': 0.98953724,\n","  'index': 2,\n","  'word': '##han',\n","  'start': 3,\n","  'end': 6},\n"," {'entity': 'B-PER',\n","  'score': 0.9915536,\n","  'index': 3,\n","  'word': '##shu',\n","  'start': 6,\n","  'end': 9},\n"," {'entity': 'I-PER',\n","  'score': 0.99238527,\n","  'index': 4,\n","  'word': 'kumar',\n","  'start': 10,\n","  'end': 15},\n"," {'entity': 'B-ORG',\n","  'score': 0.7900725,\n","  'index': 10,\n","  'word': 'in',\n","  'start': 32,\n","  'end': 34},\n"," {'entity': 'B-ORG',\n","  'score': 0.9049379,\n","  'index': 11,\n","  'word': '##eur',\n","  'start': 34,\n","  'end': 37},\n"," {'entity': 'B-ORG',\n","  'score': 0.85557437,\n","  'index': 12,\n","  'word': '##on',\n","  'start': 37,\n","  'end': 39}]"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["example=\"sunny is a founder of microsoft\"\n","nlp_pipeline(example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV6ykcWihAQy","executionInfo":{"status":"ok","timestamp":1708231249359,"user_tz":-330,"elapsed":440,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"ced0d6cd-c281-4b58-d89a-0ae3cd78581b"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-PER',\n","  'score': 0.97659636,\n","  'index': 1,\n","  'word': 'sunny',\n","  'start': 0,\n","  'end': 5},\n"," {'entity': 'B-ORG',\n","  'score': 0.8402899,\n","  'index': 6,\n","  'word': 'microsoft',\n","  'start': 22,\n","  'end': 31}]"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["example=\"apple launch mobile while eating apple which taste like orange\"\n","nlp_pipeline(example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NXJKIfPQhEh8","executionInfo":{"status":"ok","timestamp":1708231259080,"user_tz":-330,"elapsed":395,"user":{"displayName":"Naveen Vinayak","userId":"08141348341564294743"}},"outputId":"67aeb917-3e94-43c1-f600-dc5c57406fe7"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-ORG',\n","  'score': 0.87446,\n","  'index': 1,\n","  'word': 'apple',\n","  'start': 0,\n","  'end': 5},\n"," {'entity': 'B-ORG',\n","  'score': 0.3136556,\n","  'index': 6,\n","  'word': 'apple',\n","  'start': 33,\n","  'end': 38}]"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":[],"metadata":{"id":"iFTHqJ9lhG7p"},"execution_count":null,"outputs":[]}]}